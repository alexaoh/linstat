---
title: "Recommended Exercise 1 in Statistical Linear Models, Spring 2021"
author: "alexaoh"
date: "11.01.2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = "#>")
```

# Problem 1 \hspace{3mm} Simple matrix calculations

Solve the problems by hand and by use of R when possible. 

## a) 
Construct $A$ as a matrix in R. 
```{r a), eval = TRUE, echo = TRUE}
A <- matrix(c(9, -2, -2, 6), nrow = 2)
A
```

## b) 
$A$ is symmetric, which can be seen in R via 
```{r b), eval = TRUE, echo = TRUE}
t(A) == A
```

## c)
Show that $A$ is positive definite.

From the definition of positive definiteness we get
$$
\begin{pmatrix}
a & b
\end{pmatrix}\begin{pmatrix}
9 & -2\\
-2 & 6
\end{pmatrix}\begin{pmatrix}
a\\
b
\end{pmatrix} = 9a^2 - 4ab +6b^2 = 9(a^2 -\frac49ab+\frac{4}{81}b^2) + \frac{50}{9}b^2 = 9(a - \frac29b)^2 + \frac{50}{9}b^2 > 0.
$$
Hence, $A$ is positive definite. 

## d)
Find the eigenvalues and eigenvectors of $A$.
```{r d), eval = TRUE, echo = TRUE}
evalues <- eigen(A)

# Names in data frame given from eigen().
names(evalues)

# Eigenvalues.
evalues$values

# Eigenvectors.
evalues$vectors

# Check if the eigenvectors are normal.
t(evalues$vectors[, 1]) %*% evalues$vectors[, 1] 
t(evalues$vectors[, 2]) %*% evalues$vectors[, 2] 
# Since t(vector)*vector = 1 for both vectors, they are normal. 
```

## e)
Find an orthogonal diagonalization of $A$. 
```{r e), eval = TRUE, echo = TRUE}
lambda <- diag(evalues$values)
P <- evalues$vectors

# Orthogonal diagonalization.
ortho.diag <- P %*% lambda %*% t(P)
ortho.diag == A 
```

By hand it can be done in the same way. Construct a matrix $\Lambda$ which contains the eigenvalues of $A$ on its diagonal. Furthermore, a matrix $P$ is constructed by using the normalized eigenvectors as columns, in the same respective order as in $\Lambda$. The product $A = P\Lambda P^T$ is the orthogonal diagonalization of $A$.

## f)
Find $A^{-1}$.
```{r f), eval = TRUE, echo = TRUE}
A.inverse <- solve(A)
A.inverse
```
By hand it can be done by use of regular Gaussian elimination. 

## g)
Find the eigenvalues and eigenvectors of $A^{-1}$. Relationship between the eigenvalues and eigenvectors of $A$ and $A^{-1}$?
```{r g), eval = TRUE, echo = TRUE}
evalues.inverse <- eigen(A.inverse)

# Eigenvalues of A.
evalues$values
# Eigenvalues of A inverse. 
evalues.inverse$values

# Eigenvectors of A.
evalues$vectors
# Eigenvectors of A inverse.
evalues.inverse$vectors
```

It is apparent that the eigenvalues of $A$ and $A^{-1}$ are reciprocals. Furthermore, the eigenvectors have the following relationship

* The eigenvector of $A$'s eigenvalue 10 is identical to the eigenvector of $A^{-1}$'s eigenvalue $\frac{1}{10}$ (the reciprocal eigenvalue).
* The eigenvector of $A$'s eigenvalue 5 is $-1$ times the eigenvector of $A^{-1}$'s eigenvalue $\frac{1}{5}$ (the reciprocal eigenvalue).

## h)
Why can $A$ be a covariance matrix?

$A$ can be a covariance matrix since $(a_{12}) = (a_{21})$, since the covariance operator has the property $\text{Cov}(X_i, X_j) = \text{Cov}(X_j, X_i), \quad i \neq j$. 

## i)
Assume that $A$ is the covariance matrix of a random vector. Find the correlation matrix. Check the computations with `cov2cor`.

The correlation matrix is given by the following calculation in R

```{r}
R <- diag(1, nrow = 2)
rho <- function(i, j){
  return (A[i, j]/sqrt(A[i, i]%*%A[j, j]))
}

R[1, 2] <- rho(1, 2)
R[2, 1] <- rho(2, 1)

# Correlation matrix calculated manually.
R


# Correlation matrix calculated with cov2cor.
cov2cor(A)
```

It is apparent that the two matrices are identical. The same formula for $\rho_{ij}$ can be used to find the correlation matrix $R$ by hand, which has ones on the entire diagonal (naturally, since each value in the stochastic vector correlates to the fullest with itself).

## j) 

```{r}
E.X <- c(3,1)
Cov.X <- A

# First expression.
B <- matrix(c(1,1,1,2), nrow = 2)
B%*%E.X # Expectation.
B%*%Cov.X%*%t(B) # Covariance.

# Second expression.
C <- matrix(c(1,2), nrow=1)
C%*%E.X # Expectation.
C%*%Cov.X%*%t(C) # Covariance.

# Third expression (block matrix).
matrix(c(E.X, 3*E.X), nrow = 1) # Expectation.
# Not sure how to find the covariance matrix of the block matrix?


```

# Problem 2 \hspace{3mm} Mean and covariance of linear combinations

Find the mean and covariance of $A\boldsymbol{X}$ (below).
```{r}
E.X <- c(1,1,1)
Cov.X <- diag(3)

A <- matrix(c(2/3, -1/3, -1/3, -1/3, 2/3, -1/3, -1/3, -1/3, 2/3), nrow=3)

# Mean.
A%*%E.X

# Covariance.
A%*%Cov.X%*%t(A)
```


# Problem 3 \hspace{3mm} Covariance formula

Show that $\text{Cov}(\boldsymbol{V}, \boldsymbol{W}) = \text{E}(\boldsymbol{VW}^T) - (\text{E}\boldsymbol{V})(\text{E}\boldsymbol{W})^T$.

# Problem 4 \hspace{3mm} The square root matrix and the Mahalanobis transform 

## a)

## b)

## c)

## d)