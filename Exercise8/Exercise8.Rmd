---
title: "Recommended Exercise 8 in Statistical Linear Models, Spring 2021"
author: "alexaoh"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = "#>", message = F, warning = F, cache = T)
```


# Problem 1 \hspace{3mm} One- and two-way ANOVA - and the linear model

## a)
```{r}
income <- c(300, 350, 370, 360, 400, 370, 420, 
            390,400, 430, 420, 410, 300, 320, 310, 
            305,350, 370, 340, 355, 370, 380, 360, 365)
gender <- c(rep("Male", 12), rep("Female",12))
place <- rep(c(rep("A",4), rep("B",4), rep("C",4)),2)
data <- data.frame(income, gender, place)

pairs(data)
plot(income~place, data=data)
plot(income~gender, data=data)
interaction.plot(data$gender, data$place, data$income)
plot.design(income~place+gender, data=data)
```

## b)

```{r}
X <- cbind(rep(1,length(data$income)), data$place=="A", data$place=="B",data$place=="C")
XTX <- t(X) %*% X
qr(XTX)$rank
```

The rank of $X^TX$ is 3. We need it to have full rank in order to be able to estimate the coefficients in the model. Problems with non-full rank can be solved by different encodings of the coefficients, e.g. dummy coding, which is standard in R. 

## c)

```{r}
model <- lm(income~place-1, data=data, x = T)
summary(model)
anova(model)
```

The intercept is removed, which gives the parametrization in this case. This means that each coefficient estimate includes the mean, i.e. the model has no mean (it is set to zero). The null hypothesis tested in `anova` is $\alpha_A = \alpha_B = \alpha_c = 0$. The result is that the null hypothesis is discarded, because the p-value is significant, which means that the model has some merit. 

## d)

```{r}
options(contrasts=c("contr.treatment", "contr.poly"))
model1 <- lm(income~place, data=data, x=TRUE)
summary(model1)
anova(model1)

options(contrasts=c("contr.sum", "contr.poly"))
model2 <- lm(income~place, data=data, x=TRUE)
summary(model2)
anova(model2)
```

When using `contr.treatment` the regular "dummy coding" is used, i.e. placeA is dropped/merged with the intercept. Thus the coefficient estimate for placeA is found in the intercept, while the estimates for placeB and placeC are found by adding the estimates from the model to the intercept, respectively. In essence, placeA is used as a baseline. 

When using `contr.sum` the "zero-sum" or "effect coding" is used. This means that, in order to retrieve the estimate for placeA, the coefficient called place1 is added to the intercept, while, similarly, the estimate for placeB is retrieved by adding the coefficient called place2 to the intercept. The estimate for placeC can be retrieved by computing the intercept minus the other two coefficients (place1 and place2). 

## e)

```{r}
# Model 1
r <- 2
C <- cbind(rep(0,r), diag(r))
C

d <- matrix(rep(0,r), ncol=1)
n <- length (data$income)

betahat <- matrix(model1$coefficients, ncol=1)
sigma2hat <- summary(model1)$sigma^2
X <- model.matrix(model1)
F1 <- (t(C%*%betahat-d)%*%solve(C%*%solve(t(X)%*%X)%*%t(C))%*%(C%*%betahat-d))/(r*sigma2hat)
F1
1-pf(F1,r,n-length(betahat))

# Model 2
betahat2 <- matrix(model2$coefficients, ncol=1)
sigma2hat2 <- summary(model2)$sigma^2
X2 <- model.matrix(model2)
F2 <- (t(C%*%betahat2-d)%*%solve(C%*%solve(t(X2)%*%X2)%*%t(C))%*%(C%*%betahat2-d))/(r*sigma2hat2)
F2
1-pf(F2,r,n-length(betahat))
```

We can see that the test for both models gives the same result!

## f)

```{r}
options(contrasts=c("contr.treatment", "contr.poly"))
model3 <- lm(income~place+gender, data=data, x=TRUE)
anova(model3)
summary(model3)

options(contrasts=c("contr.sum", "contr.poly"))
model4 <- lm(income~place+gender, data=data, x=TRUE)
summary(model4)
anova(model4)
```

As before, the first model uses the "dummy coding", while the second model uses the "effect coding". Also, the ANOVA-tables look the same for both models, as expected based on the One-way ANOVA from earlier. 

```{r}
interaction.model <- lm(income~place*gender, data = data, x = TRUE) 
summary(interaction.model)
anova(interaction.model)

# Manual F-test on the interaction model
r <- 2
C2 <- matrix(c(0,0,0,0,1,0,0,0,0,0,0,1), byrow = T, nrow = 2)
C2

d <- matrix(rep(0,r), ncol=1)
n <- length (data$income)

betahat3 <- matrix(interaction.model$coefficients, ncol=1)
sigma2hat3 <- summary(interaction.model)$sigma^2
X3 <- model.matrix(interaction.model)
F3 <- (t(C2%*%betahat3-d)%*%solve(C2%*%solve(t(X3)%*%X3)%*%t(C2))%*%(C2%*%betahat3-d))/(r*sigma2hat3)
F3
1-pf(F3,r,n-length(betahat3))
```


As is apparent, the interaction effect is not significant according to the F-test. 

# Problem 2 \hspace{3mm} Teaching reading

## a)

Define $\mu_A, \mu_B$ and $\mu_C$ as the expected score when using methods $A, B$ and $C$ respectively. 

Then, the hypothesis test is 

$$
H_0: \mu_A = \mu_B = \mu_C \hspace{2mm} \text{ vs. } \hspace{2mm} H_1: \text{ At least one is different from the others}.
$$

The *treatment sum of squares*, i.e. the SSR is given by $22\cdot((41.05-44.02)^2 + (44.27-44.02)^2 + (46.73-44.02)^2) = 357.005$ and the SSE is 2511.712. 

The model under the null hypothesis only contains an intercept, where $\text{SSE}_0$ is the same as SST, since SSR vanishes. Thus the F-test reads

$$
F = \frac{(\text{SSE}_0-\text{SSE})/r}{\text{SSE}/(n-p)} = \frac{\text{SSR}/r}{\text{SSE}/(n-p)} = \frac{357.005/2}{2511.712/(66-3)} = 4.477.
$$

The p-value is 0.015, so the null hypothesis is rejected (at least) at level 0.05, i.e. the teaching method matters.  

Assumptions needed to perform the test are that the model has the form $X_{ij} = \mu + \alpha_i + \epsilon_{ij}$, for $i = 1,2,3$ and $j = 1, 2, \dots, 22$, where the response is the reading score for subject $j$ receiving teaching method $i$. 

## b)

Define $\gamma = \mu_B/\mu_C$. We suggest an estimator for this quantity: $\hat{\gamma} = \bar{X}_B/\bar{X}_C$. A first-order Taylor expansion yields 

$$
\frac{x}{y} = h(x,y) \approx h(\mu_B, \mu_C) + h_x(\mu_B, \mu_C)(x-\mu_B) + h_y(\mu_B, \mu_C)(y-\mu_C) = \frac{\mu_B}{\mu_C} + \frac{1}{\mu_C}(x-\mu_C) - \frac{\mu_B}{\mu_C^2}(y-\mu_C), 
$$

which implies the following approximations to the expected value and standard deviation of this estimator 

$$
\begin{split}
\widehat{\text{E}(\hat{\gamma})} &= \frac{\hat{\mu}_B}{\hat{\mu}_C} \\
\widehat{\text{SD}(\hat{\gamma})} &= \left(\frac{1}{\hat{\mu}_C^2} \text{Var}(\bar{X}_B) + \frac{\hat{\mu}_B^2}{\hat{\mu}_C^4}\text{Var}(\bar{X}_C)\right)^{1/2} = \frac{1}{\hat{\mu}_c}\sqrt{\frac{\hat{\sigma}^2_B}{n_B}+ \frac{\hat{\mu}_B^2}{\hat{\mu}_C^2}\frac{\hat{\sigma}^2_C}{n_C}}, 
\end{split}
$$

where $\text{Var}(\bar{X}_B) = \frac{\sigma_B^2}{n_B}$. With the given numerical values, these estimates are $\widehat{\text{E}(\hat{\gamma})} = \frac{46.73}{44.27} \approx 1.06$ and $\widehat{\text{SD}(\hat{\gamma})} = \frac{1}{44.27}\sqrt{\frac{7.388^2}{22}+ \frac{46.73^2}{44.27^2}\frac{5.767^2}{22}} \approx 0.046$.
