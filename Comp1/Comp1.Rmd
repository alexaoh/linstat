---
title: "Compulsory Exercise 1 in Statistical Linear Models, Spring 2021"
author: "Sander Ruud, Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = "#>", warning = FALSE, message=FALSE)
```

# Problem 1 \hspace{3mm} Bivariate normal distribution

## a)

Let $$A = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}.$$ Then

$$
\text{E}(\boldsymbol{Y}) = \text{E}(A\boldsymbol{X}) = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \text{E}(\boldsymbol{X}) = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \begin{pmatrix}
0 \\
2 \\
\end{pmatrix} = \begin{pmatrix}
-2/\sqrt{2} \\
2/\sqrt{2} \\
\end{pmatrix} = \sqrt{2}\begin{pmatrix}
-1 \\
1 \\
\end{pmatrix}
$$

and 

$$
\begin{split}
\text{Cov}(\boldsymbol{Y}) = \text{Cov}(A\boldsymbol{X}) = A\text{Cov}(\boldsymbol{X})A^T &= \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \begin{pmatrix}
3 & 1 \\
1 & 3 \\
\end{pmatrix}\begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}^T \\ &= \begin{pmatrix}
2/\sqrt{2} & -2/\sqrt{2} \\
4/\sqrt{2} & 4/\sqrt{2} \\
\end{pmatrix}\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
-1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 \\
0 & 4 \\
\end{pmatrix}
\end{split}
$$

We know that the Gaussian distribution is closed under linear combinations. This means that 

$$
\mathbf{Y} \sim N_{2}\left(\sqrt{2}\begin{pmatrix}
-1 \\
1 \\
\end{pmatrix}, \begin{pmatrix}
2 & 0 \\
0 & 4 \\
\end{pmatrix}\right)
$$ 

Moreover, since we know that uncorrelated variables in a random vector with a multivariate Gaussian distribution must be independent, this means that $Y_1$ and $Y_2$ are independent. 

Bonus: Calculations in R.

```{r}
mu.X <- c(0,2)
Sigma.X <- matrix(c(3,1,1,3), ncol = 2)
A <- matrix(c(1/sqrt(2), 1/sqrt(2), -1/sqrt(2), 1/sqrt(2)), ncol = 2)

mu.Y <- A %*% mu.X
mu.Y

Sigma.Y <- A %*% Sigma.X %*% t(A)
Sigma.Y  

# Yes, the coordinates of Y are independent, since Y is normally distributed 
# and the coordinates are uncorrelated.
```


## b)

By using the method of diagonalization one can show that the contours of a vector that has a multivariate Gaussian distribution are ellipsoids. Moreover, one can show that the axes of the ellipsoids have direction along the eigenvector of the covariance matrix $\Sigma$ and half-lengths $\sqrt{\lambda_i}b$ for, where $\lambda_i$ are the eigenvalues of $\Sigma$. More specifically, each of the half axes are given, in descending order based on length, by $\sqrt{\lambda_i}b$ with direction following each respective eigenvector $\mathbf{p}_i, \, i = 1, \dots, k$ for $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_k$, where $k$ is the dimension of the multivariate Gaussian distribution. 

\textcolor{red}{Jeg spurte om den matematiske utledningen med diagonalisering skal med her, og han sa at den kunne det hvis vi ville, men at de egentlig ikke så etter det. Hvis vi mener det forklarer mer, så kunne vi ta det med. Tanker?}

The probability that $\boldsymbol{X}$ falls within the given ellipse is

$$
\begin{split}
&P\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2} \leq b^2\right), \quad \text{where the change of variables } P\mathbf{y} = \mathbf{x}-\mathbf{\mu} \quad \text{is implemented} \\
= &P\left(\frac{y_1^2\lambda_2 + y_2^2\lambda_1}{\lambda_1\lambda_2} \leq 4.6^2\right) \quad \text{Denne er sikkert unødvendig}\\
= &\text{Tror ikke dette fører noe sted.}
\end{split}
$$

Theorem B8 in FKLM states that, in general, $\boldsymbol{Y} = (\boldsymbol{X}-\boldsymbol{\mu})\Sigma^{-1}(\boldsymbol{X}-\boldsymbol{\mu}) \sim \chi^2_2$ when $\boldsymbol{Y} \sim N_{2}(\mu, \Sigma)$. Hence, the probability that $\boldsymbol{X}$ falls within the given ellipse is $\approx 0.9$, which is seen from the given output of `qchisq(0.9, 2)` $=$ `r qchisq(0.9, 2)`. This means that the quantile with the value $b \approx 4.6$ corresponds to the probability $\approx 0.9$.

COMMENT: Denne [stackoverflow](https://math.stackexchange.com/questions/3543987/multivariate-normal-probability-of-being-inside-ellipse) kan være til hjelp også (samme oppgave lol).

<!-- Sander -->
**Skjønner ikke hva du har gjort over**
We want the ellipsoid that captures information about the covariates in a way that a certain amount of points drawn from our distribution will end up inside this ellipsoid.

```{r echo=TRUE, eval=TRUE, fig.height=6, fig.width=6}
library(mixtools)
sigma=matrix(c(3,1,1,3),ncol=2)
mu_x <- 0
mu_y <- 2

lam1 <- eigen(sigma)$values[1]
v1 <- eigen(sigma)$vectors[,1]

lam2 <- eigen(sigma)$values[2]
v2 <- eigen(sigma)$vectors[,2]

l1 = 2*sqrt(qchisq(0.9,2)) * sqrt(lam1)
l2 = 2*sqrt(qchisq(0.9,2)) * sqrt(lam2)

x1 = (v1/length(v1)*l1)[1]
x2 = (v1/length(v1)*l1)[2]

x3 = (v2/length(v2)*l2)[1]
x4 = (v2/length(v2)*l2)[2]

ellipse(c(mu_x, mu_y), sigma, alpha = 0.1, newplot=TRUE, type='l')
lines(c(mu_x, x1+mu_x), c(mu_y, x2+mu_y))
lines(c(mu_x, x3+mu_x), c(mu_y, x4+mu_y))

```


\textcolor{red}{Vi må spørre noen om hvorfor det er ganget med 2, f.eks i øvingstimen på fredag!! Det gir ikke mening i det hele tatt synes jeg!?}

$90$% of the points generated by this distribution will be expected to stay inside the elipsoid. 4.6 is the threshold when you have 2 degrees of freedom

\textcolor{red}{Half-axes represent eigenvectors with sqrt(eigenvalue) length, I'll make figures nice later.}

<!-- Sander -->

# Problem 2 \hspace{3mm} Distributional results for $\bar{X}$ and $S^2$ for a univariate normal sample

## a)

$$
\overline{X} = \frac1n\boldsymbol{1}^T\boldsymbol{X} = \frac1n\begin{pmatrix} 
    1 \, \cdots \, 1
    \end{pmatrix}\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix}=\frac1n\sum_{i=1}^nX_i.
$$

Considering the $i^{th}$ component of $\boldsymbol{X}^TC\boldsymbol{X}$ gives (gidder ikke mer nå)

$$
\begin{split}
\begin{pmatrix} 
    X_1 \, \cdots \, X_n
\end{pmatrix}(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)_i\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix}
\end{split}
$$

We could also, instead of going the route above, show that $\boldsymbol{X} - \frac1n\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{X} = (I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X} = \sum_{i=1}^n\overline{X} ???$ and use the proof below. This can be shown by

$$
\begin{split}
\boldsymbol{X} - \frac1n\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{X} &= \boldsymbol{X}- \begin{pmatrix} 
    1-1/n & -1/n & \dots & -1/n\\
    \vdots & \ddots & \vdots & \vdots \\
    -1/n & -1/n & \dots & 1-1/n
    \end{pmatrix}\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix} = \boldsymbol{X} - \begin{pmatrix} 
    X_1 - \frac{X_1}{n} - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    -\frac{X_1}{n} + X_2 - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    \vdots \\
    -\frac{X_1}{n} - \frac{X_2}{n} - \dots + X_n - \frac{X_n}{n} 
    \end{pmatrix} \\
    &= \begin{pmatrix} 
    X_1\\
    \vdots\\
    X_n
    \end{pmatrix} - \begin{pmatrix} 
    X_1 - \frac{X_1}{n} - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    -\frac{X_1}{n} + X_2 - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    \vdots \\
    -\frac{X_1}{n} -\frac{X_2}{n} - \dots + X_n - \frac{X_n}{n} 
    \end{pmatrix} = \begin{pmatrix} 
    \sum_{i = 1}^{n} \frac{X_i}{n} \\
    \sum_{i = 1}^{n} \frac{X_i}{n}\\
    \vdots \\
    \sum_{i = 1}^{n} \frac{X_i}{n}
    \end{pmatrix}. 
\end{split}
$$

This means that 

$$
((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X})^T((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X}) = \begin{pmatrix} 
    \sum_{i = 1}^{n} \frac{X_i}{n} &
    \sum_{i = 1}^{n} \frac{X_i}{n} &
    \dots &
    \sum_{i = 1}^{n} \frac{X_i}{n}
    \end{pmatrix}\begin{pmatrix} 
    \sum_{i = 1}^{n} \frac{X_i}{n} \\
    \sum_{i = 1}^{n} \frac{X_i}{n}\\
    \vdots \\
    \sum_{i = 1}^{n} \frac{X_i}{n}
    \end{pmatrix}
$$

$$
\begin{split}
S^2 &= \frac{1}{n-1}\boldsymbol{X}^TC\boldsymbol{X} = \frac{1}{n-1}\boldsymbol{X}^T(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X} \\
&= \frac{1}{n-1}((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X})^T((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X}) \text{Det er denne til neste jeg ikke klarer å vise, som jeg prøver å vise over!}\\
&= \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})(X_i-\overline{X}) = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2,
\end{split}
$$

since $(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)$ is idempotent and symmetric. The fact that $C$ is idempotent and symmetric may be verified by direct calculation. The idempotent property follows by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)^2 & = (I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T) = I - \frac2n\boldsymbol{1}\boldsymbol{1}^T + \frac{1}{n^2}\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T \\
&= I - \frac2n\boldsymbol{1}\boldsymbol{1}^T + \frac1n\boldsymbol{1}\boldsymbol{1}^T = I - \boldsymbol{1}\boldsymbol{1}^T, 
\end{split}
$$

where the third equality follows since, in $\mathbb{R}^n$, we have that $\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T = n\boldsymbol{1}\boldsymbol{1}^T$. The symmetric property follows by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)^T = I^T-\frac1n(\boldsymbol{1}\boldsymbol{1}^T)^T = I-\frac1n\boldsymbol{1}\boldsymbol{1}^T.
\end{split}
$$

Hence, the proof above is correct based on the (correct) assumption that the centering matrix is idempotent and symmetric. 


DETTE ER LITT KAOS NÅ, MEN KAN FIKSES OPP I SENERE. 

## b)

$$
\frac{1}{n}\boldsymbol{1}^TC=\frac{1}{n}\boldsymbol{1}^TI-\frac{1}{n^2}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T=\boldsymbol{1}^T\frac{1}{n}-\frac{1}{n^2}\left[n\: ...\: n\right]=\boldsymbol{1}^T(1/n-1/n)=\boldsymbol{0}^T
$$

By Corollary 5.2 (HS), this implies that $\frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X} \text { and } C \boldsymbol{X}$ are independent, since $\frac{1}{n}\boldsymbol{1}^T\sigma IC^T = 0$, $C$ is idempotent and $\boldsymbol{X} \sim N(\mu \boldsymbol{1}, \sigma^2I)$. This is a result of the fact that two linear transforms, $A\boldsymbol{X}$ and $B\boldsymbol{X}$, of a multivariate Gaussian $\boldsymbol{X}$ are independent iff $\text{Cov}(A\boldsymbol{X}, B\boldsymbol{X}) = 0$.
Furthermore, this also implies that $\bar{X}$ and $S^{2}$ are independent by theorem B.8.2 (FKLM) \textcolor{red}{Tror det er nettopp dette de vil at vi skal vise. Viser her:}. Since $\frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X} \text { and } C \boldsymbol{X}$ are independent, $\frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X} \text { and } (C\boldsymbol{X})^T C \boldsymbol{X} = \boldsymbol{X}^T C^2 \boldsymbol{X} = \boldsymbol{X}^T C^2 \boldsymbol{X}$ must also be independent. This implies that $\bar{X}$ and $S^{2} = \frac{1}{n-1}\boldsymbol{X}^TC\boldsymbol{X}$ are independent. 

## c)

Theorem B.8.2 (FKLM), also tells us that

$$
\frac{n-1}{\sigma^{2}} S^{2} \sim \chi_{n-1}^{2}
$$

Suppose $X_1,...,X_n$ is a univariate random sample with mean $\mu$. Then 


$$
\frac{n-1}{\sigma^{2}} S^{2} = \frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)^2
$$


$$
\frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\mu \right)^2=\frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X}+\bar{X}-\mu \right)^2
$$
Expanding and using the definition of $\bar{X}$,

$$
\frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)^2+\frac{2}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)\bar{X}+
\frac{1}{\sigma^2}\sum_{i=1}^n \bar{X}^2 = \frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)^2 +\frac{n}{\sigma^2}(\bar{X}-\mu)^2
$$

Furthermore, we know that $\sum_{i=1}^{n}\left(\frac{X_i-\mu}{\sigma}   \right)^2 \sim \chi_n^2$, $\bar{X}\sim N(\mu, \frac{\sigma^2}{n})$ and $\frac{n}{\sigma^2}(\bar{X}-\mu)^2\sim \chi_1^2$.

Because we have the sum of two independent distributions, their combined moment generating function is the product of their individual moment generating functions, $a=b+c\Rightarrow M_a=M_b M_c$, which here results in

$$
M(t) = (1-2t)^{-\frac{n}{2}} / (1-2t)^{-\frac{1}{2}}=(1-2t)^{-(n-1)/2}
$$
Because, two distributions with the same MGF have the same distribution, we recognize this as a $\chi_{n-1}^2$ distrubution. $\square$






