---
title: "Compulsory Exercise 1 in TMA4267 Statistical Linear Models, Spring 2021"
author: "Sander Ruud, Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = "#>", warning = FALSE, message=FALSE)
```

# Problem 1 \hspace{3mm} Bivariate normal distribution

## a)

Let $$A := \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}.$$ Then

$$
\text{E}(\boldsymbol{Y}) = \text{E}(A\boldsymbol{X}) = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \text{E}(\boldsymbol{X}) = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \begin{pmatrix}
0 \\
2 \\
\end{pmatrix} = \begin{pmatrix}
-2/\sqrt{2} \\
2/\sqrt{2} \\
\end{pmatrix} = \sqrt{2}\begin{pmatrix}
-1 \\
1 \\
\end{pmatrix}
$$

and 

$$
\begin{split}
\text{Cov}(\boldsymbol{Y}) = \text{Cov}(A\boldsymbol{X}) = A\text{Cov}(\boldsymbol{X})A^T &= \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \begin{pmatrix}
3 & 1 \\
1 & 3 \\
\end{pmatrix}\begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}^T \\ &= \begin{pmatrix}
2/\sqrt{2} & -2/\sqrt{2} \\
4/\sqrt{2} & 4/\sqrt{2} \\
\end{pmatrix}\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
-1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 \\
0 & 4 \\
\end{pmatrix}
\end{split}
$$

We know that the Gaussian distribution is closed under linear combinations. This means that 

$$
\mathbf{Y} \sim N_{2}\left(\sqrt{2}\begin{pmatrix}
-1 \\
1 \\
\end{pmatrix}, \begin{pmatrix}
2 & 0 \\
0 & 4 \\
\end{pmatrix}\right)
$$ 

Moreover, since we know that uncorrelated variables in a random vector with a multivariate Gaussian distribution must be independent, this means that $Y_1$ and $Y_2$ are independent. 

Bonus: Calculations in R.

```{r}
mu.X <- c(0,2)
Sigma.X <- matrix(c(3,1,1,3), ncol = 2)
A <- matrix(c(1/sqrt(2), 1/sqrt(2), -1/sqrt(2), 1/sqrt(2)), ncol = 2)

mu.Y <- A %*% mu.X
mu.Y

Sigma.Y <- A %*% Sigma.X %*% t(A)
Sigma.Y  

# Yes, the coordinates of Y are independent, since Y is normally distributed 
# and the coordinates are uncorrelated.
```


## b)

By using the method of diagonalization one can show that the contours of a vector that has a multivariate Gaussian distribution are ellipsoids. Moreover, one can show that the axes of the ellipsoids have direction along the eigenvectors of the covariance matrix $\Sigma$ and half-lengths $\sqrt{\lambda_ib}$ for, where $\lambda_i$ are the eigenvalues of $\Sigma$. More specifically, each of the half axes are given, in descending order based on length, by $\sqrt{\lambda_ib}$ with direction following each respective eigenvector $\mathbf{p}_i, \, i = 1, \dots, k$ for $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_k$, where $k$ is the dimension of the multivariate Gaussian distribution. 

<!-- The probability that $\boldsymbol{X}$ falls within the given ellipse is -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- &P\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2} \leq b^2\right), \quad \text{where the change of variables } P\mathbf{y} = \mathbf{x}-\mathbf{\mu} \quad \text{is implemented} \\ -->
<!-- = &P\left(\frac{y_1^2\lambda_2 + y_2^2\lambda_1}{\lambda_1\lambda_2} \leq 4.6^2\right) \quad \text{Denne er sikkert unødvendig}\\ -->
<!-- = &\text{Tror ikke dette fører noe sted.} -->
<!-- \end{split} -->
<!-- $$ -->

Theorem B.8.1 (FKLM, 2013) states that, in general, $\boldsymbol{Y} = (\boldsymbol{X}-\boldsymbol{\mu})\Sigma^{-1}(\boldsymbol{X}-\boldsymbol{\mu}) \sim \chi^2_2$ when $\boldsymbol{Y} \sim N_{2}(\mu, \Sigma)$. Hence, the probability that $\boldsymbol{X}$ falls within the given ellipse is $\approx 0.9$, which is seen from the given output of `qchisq(0.9, 2)` $=$ `r qchisq(0.9, 2)`. This means that the quantile in the $\chi^2_2$-distribution with the value $b \approx 4.6$ corresponds to the probability $\approx 0.9$.

<!-- COMMENT: Denne [stackoverflow](https://math.stackexchange.com/questions/3543987/multivariate-normal-probability-of-being-inside-ellipse) kan være til hjelp også (samme oppgave lol). -->

We want the ellipsoid that captures information about the covariates in a way that a certain amount of points drawn from our distribution will end up inside this ellipsoid. The ellipsoid that includes $90 \, \%$ of random points generated from the bivariate normal distribution of $\boldsymbol{X}$ is drawn below. 

```{r echo=TRUE, eval=TRUE, fig.height=4.6, fig.width=4, fig.align="center"}
library(mixtools)
library(latex2exp)
sigma=matrix(c(3,1,1,3),ncol=2)
mu_x <- 0
mu_y <- 2

lam1 <- eigen(sigma)$values[1]
v1 <- eigen(sigma)$vectors[,1]

lam2 <- eigen(sigma)$values[2]
v2 <- eigen(sigma)$vectors[,2]

l1 = sqrt(qchisq(0.9,2)*lam1)
l2 = sqrt(qchisq(0.9,2)*lam2)

# The vectors are already normalized in R.
x1 = (v1*l1)[1]
x2 = (v1*l1)[2]

x3 = (v2*l2)[1]
x4 = (v2*l2)[2]

ellipse(c(mu_x, mu_y), sigma, alpha = 0.1, newplot=TRUE, type='l')
lines(c(mu_x, x1+mu_x), c(mu_y, x2+mu_y), col = "blue")
text(2,2.7, labels=TeX("$\\sqrt{b\\lambda_1}$"))
lines(c(mu_x, x3+mu_x), c(mu_y, x4+mu_y), col = "red")
text(-1.8,2.2, labels=TeX("$\\sqrt{b\\lambda_2}$"))
```

The major half-axis, in blue, shows $\sqrt{\lambda_1b}\frac{\boldsymbol{p_1}}{|\boldsymbol{p_1}|}$, i.e. the length $\sqrt{\lambda_1b}$ in the direction of the first eigenvector of $\Sigma$. Similarly, the minor half-axis, in red, shows $\sqrt{\lambda_2b}\frac{\boldsymbol{p_2}}{|\boldsymbol{p_2}|}$.

# Problem 2 \hspace{3mm} Distributional results for $\overline{X}$ and $S^2$ for a univariate normal sample

## a)

First, it will be shown that $\overline{X} := \frac1n\sum_{i=1}^nX_i = \frac1n\boldsymbol{1}^T\boldsymbol{X}$

$$
\overline{X} = \frac1n\boldsymbol{1}^T\boldsymbol{X} = \frac1n\begin{pmatrix} 
    1 \, \cdots \, 1
    \end{pmatrix}\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix}=\frac1n\sum_{i=1}^nX_i.
$$

Next, it will be shown that $S^2 := \frac{1}{n-1}\sum_{i=1}^n(X_i - \overline{X})^2 = \frac{1}{n-1}\boldsymbol{X}^TC\boldsymbol{X}$, where $C = I-\frac1n\boldsymbol{1}\boldsymbol{1}^T$. To this end, we will first show that $\boldsymbol{X} - \frac1n\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{X} = (I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X} = \boldsymbol{X} - \overline{\boldsymbol{X}}$, where $\overline{\boldsymbol{X}}$ is a vector of the average of the components of $\boldsymbol{X}$. This can be shown by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X} &= \begin{pmatrix} 
    1-1/n & -1/n & \dots & -1/n\\
    \vdots & \ddots & \vdots & \vdots \\
    -1/n & -1/n & \dots & 1-1/n
    \end{pmatrix}\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix} = \begin{pmatrix} 
    X_1 - \frac{X_1}{n} - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    -\frac{X_1}{n} + X_2 - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    \vdots \\
    -\frac{X_1}{n} - \frac{X_2}{n} - \dots + X_n - \frac{X_n}{n} 
    \end{pmatrix} \\
    &= \begin{pmatrix} 
    X_1\\
    \vdots\\
    X_n
    \end{pmatrix} + \begin{pmatrix} 
    - \frac{X_1}{n} - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    -\frac{X_1}{n} - \frac{X_2}{n} - \dots - \frac{X_n}{n}\\
    \vdots \\
    -\frac{X_1}{n} -\frac{X_2}{n} - \dots - \frac{X_n}{n} 
    \end{pmatrix} = \boldsymbol{X} - \begin{pmatrix} 
    \sum_{i = 1}^{n} \frac{X_i}{n} \\
    \sum_{i = 1}^{n} \frac{X_i}{n}\\
    \vdots \\
    \sum_{i = 1}^{n} \frac{X_i}{n}
    \end{pmatrix} = \boldsymbol{X} - \overline{\boldsymbol{X}}.  
\end{split}
$$

This means that 

$$
((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X})^T((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X}) = (\boldsymbol{X} - \overline{\boldsymbol{X}})^T(\boldsymbol{X} - \overline{\boldsymbol{X}}) = \sum_{i=1}^n(X_i - \overline{X})^2, 
$$

since $\boldsymbol{a}^T\boldsymbol{b} = \sum_{i=1}^na_ib_i$ is the inner product between two vectors $\boldsymbol{a} = (a_1 \,\, a_2 \,\, \dots \,\, a_n)^T$ and $\boldsymbol{b} = (b_1 \,\, b_2 \,\, \dots \,\, b_n)^T$. Hence, this leads to the final result

$$
\begin{split}
S^2 &= \frac{1}{n-1}\boldsymbol{X}^TC\boldsymbol{X} = \frac{1}{n-1}\boldsymbol{X}^T(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X} \\
&= \frac{1}{n-1}((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X})^T((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X}) \\
&= \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})(X_i-\overline{X}) = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2,
\end{split}
$$

since $C = I-\frac1n\boldsymbol{1}\boldsymbol{1}^T$ is idempotent and symmetric. The fact that $C$ is idempotent and symmetric may be verified by direct calculation. The idempotent property follows by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)^2 & = (I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T) = I - \frac2n\boldsymbol{1}\boldsymbol{1}^T + \frac{1}{n^2}\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T \\
&= I - \frac2n\boldsymbol{1}\boldsymbol{1}^T + \frac1n\boldsymbol{1}\boldsymbol{1}^T = I - \frac1n\boldsymbol{1}\boldsymbol{1}^T, 
\end{split}
$$

where the third equality follows since, in $\mathbb{R}^n$, we have that $\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T = n\boldsymbol{1}\boldsymbol{1}^T$. The symmetric property follows by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)^T = I^T-\frac1n(\boldsymbol{1}\boldsymbol{1}^T)^T = I-\frac1n\boldsymbol{1}\boldsymbol{1}^T.
\end{split}
$$

## b)

$$
\frac{1}{n}\boldsymbol{1}^TC=\frac{1}{n}\boldsymbol{1}^TI-\frac{1}{n^2}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T=\boldsymbol{1}^T\frac{1}{n}-\frac{1}{n^2}\left[n\: ...\: n\right]=\boldsymbol{1}^T(1/n-1/n)=\boldsymbol{0}^T
$$

By Corollary 5.2 (HS), this implies that $\frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X} \text { and } C \boldsymbol{X}$ are independent, since $\frac{1}{n}\boldsymbol{1}^T\sigma^2 IC^T = 0$, $C$ is idempotent and $\boldsymbol{X} \sim N(\mu \boldsymbol{1}, \sigma^2I)$. This is a result of the fact that two linear transforms, $A\boldsymbol{X}$ and $B\boldsymbol{X}$, of a multivariate Gaussian $\boldsymbol{X}$ are independent iff $\text{Cov}(A\boldsymbol{X}, B\boldsymbol{X}) = 0$. Since $\frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X} \text { and } C \boldsymbol{X}$ are independent, $\frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X} \text { and } (C\boldsymbol{X})^T C \boldsymbol{X} = \boldsymbol{X}^T C^2 \boldsymbol{X} = \boldsymbol{X}^T C \boldsymbol{X}$ must also be independent. This implies that $\bar{X} = \frac{1}{n} \mathbf{1}^{\mathrm{T}} \boldsymbol{X}$ and $S^{2} = \frac{1}{n-1}\boldsymbol{X}^TC\boldsymbol{X}$ are independent. 

## c)

<!-- Theorem B.8.2 (FKLM) tells us that -->

<!-- $$ -->
<!-- \frac{n-1}{\sigma^{2}} S^{2} \sim \chi_{n-1}^{2} -->
<!-- $$ -->

<!-- Suppose $X_1,...,X_n$ is a univariate random sample with mean $\mu$ \textcolor{red}{Hva betyr dette?}. Then  -->


<!-- $$ -->
<!-- \frac{n-1}{\sigma^{2}} S^{2} = \frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)^2 -->
<!-- $$ -->


<!-- $$ -->
<!-- \frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\mu \right)^2=\frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X}+\bar{X}-\mu \right)^2 -->
<!-- $$ -->
<!-- Expanding and using the definition of $\bar{X}$, -->

<!-- $$ -->
<!-- \frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)^2+\frac{2}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)\bar{X}+ -->
<!-- \frac{1}{\sigma^2}\sum_{i=1}^n \bar{X}^2 = \frac{1}{\sigma^2}\sum_{i=1}^n \left(X_i-\bar{X} \right)^2 +\frac{n}{\sigma^2}(\bar{X}-\mu)^2 -->
<!-- $$ -->

<!-- Furthermore, we know that $\sum_{i=1}^{n}\left(\frac{X_i-\mu}{\sigma}   \right)^2 \sim \chi_n^2$, $\bar{X}\sim N(\mu, \frac{\sigma^2}{n}) \text{\textcolor{red}{Central Limit Theorem?`}}$ and $\frac{n}{\sigma^2}(\bar{X}-\mu)^2\sim \chi_1^2$. -->

<!-- Because we have the sum of two independent distributions, their combined moment generating function is the product of their individual moment generating functions, $a=b+c\Rightarrow M_a=M_b M_c$, which here results in -->

<!-- $$ -->
<!-- M(t) = (1-2t)^{-\frac{n}{2}} / (1-2t)^{-\frac{1}{2}}=(1-2t)^{-(n-1)/2} -->
<!-- $$ -->
<!-- Because, two distributions with the same MGF have the same distribution, we recognize this as a $\chi_{n-1}^2$ distribution. $\square$ -->

Using the Mahalanobis transform, we know that 

$$
\frac{1}{\sigma^2}(\boldsymbol{X}-\mu\boldsymbol{1})^TC(\boldsymbol{X}-\mu\boldsymbol{1}) \sim \chi^2_r, 
$$

where $r = \text{rank}(C)$, since $\frac{1}{\sigma^2}(\boldsymbol{X}-\mu\boldsymbol{1}) \sim N(\boldsymbol{0}, I)$ and $C = I-\frac1n\boldsymbol{1}\boldsymbol{1}^T$ is idempotent and symmetric (Theorem B.8.2, FKLM, 2013). We want to derive the distribution of $\frac{(n-1)S^2}{\sigma^2} = \frac{(n-1)}{(n-1)\sigma^2}\boldsymbol{X}^TC\boldsymbol{X} = \frac{1}{\sigma^2}\boldsymbol{X}^TC\boldsymbol{X}$. Expanding $\frac{1}{\sigma^2}(\boldsymbol{X}-\mu\boldsymbol{1})^TC(\boldsymbol{X}-\mu\boldsymbol{1})$ gives

$$
\begin{split}
&\frac{1}{\sigma^2}(\boldsymbol{X}-\mu\boldsymbol{1})^TC(\boldsymbol{X}-\mu\boldsymbol{1}) \\
= &\frac{1}{\sigma^2}(\boldsymbol{X}^TC\boldsymbol{X} - \boldsymbol{X}^TC\mu\boldsymbol{1} - (\mu\boldsymbol{1})^TC\boldsymbol{X} + (\mu\boldsymbol{1})^TC\mu\boldsymbol{1}) \\
= &\frac{1}{\sigma^2}(\boldsymbol{X}^TC\boldsymbol{X} - 2\mu\boldsymbol{X}^TC\boldsymbol{1} + \mu^2\boldsymbol{1}^TC\boldsymbol{1}) \\
= &\frac{1}{\sigma^2}\boldsymbol{X}^TC\boldsymbol{X} \\
\end{split}
$$

where the third inequality holds because $C\boldsymbol{1} = (I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{1} = \boldsymbol{1}-\frac1n\boldsymbol{1}n = 0$. Hence, since $\frac{1}{\sigma^2}(\boldsymbol{X}-\mu\boldsymbol{1})^TC(\boldsymbol{X}-\mu\boldsymbol{1}) \sim \chi^2_r$, we now know that $\frac{(n-1)S^2}{\sigma^2} = \frac{1}{\sigma^2}\boldsymbol{X}^TC\boldsymbol{X} \sim \chi^2_r$ also, where $r = \text{rank}(C) = \text{rank}(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T) = \text{tr}(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T) = n - \frac{n}{n} = n-1$. Here, $\text{tr}(C) = \text{rank}(C)$ since $C$ is idempotent. $\square$
