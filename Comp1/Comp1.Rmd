---
title: "Compulsory Exercise 1 in Statistical Linear Models, Spring 2021"
author: "Sander Ruud, Alexander J Ohrt"
date: "12.02.2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = "#>")
```

# Problem 1 \hspace{3mm} Bivariate normal distribution

## a)

Let $$A = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}.$$ Then

$$
\text{E}(\boldsymbol{Y}) = \text{E}(A\boldsymbol{X}) = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \text{E}(\boldsymbol{X}) = \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \begin{pmatrix}
0 \\
2 \\
\end{pmatrix} = \begin{pmatrix}
-2/\sqrt{2} \\
2/\sqrt{2} \\
\end{pmatrix} = \sqrt{2}\begin{pmatrix}
-1 \\
1 \\
\end{pmatrix}
$$

and 

$$
\begin{split}
\text{Cov}(\boldsymbol{Y}) = \text{Cov}(A\boldsymbol{X}) = A\text{Cov}(\boldsymbol{X})A^T &= \begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} \begin{pmatrix}
3 & 1 \\
1 & 3 \\
\end{pmatrix}\begin{pmatrix}
1/\sqrt{2} & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix}^T \\ &= \begin{pmatrix}
2/\sqrt{2} & -2/\sqrt{2} \\
4/\sqrt{2} & 4/\sqrt{2} \\
\end{pmatrix}\begin{pmatrix}
1/\sqrt{2} & 1/\sqrt{2} \\
-1/\sqrt{2} & 1/\sqrt{2} \\
\end{pmatrix} = \begin{pmatrix}
2 & 0 \\
0 & 4 \\
\end{pmatrix}
\end{split}
$$

We know that the Gaussian distribution is closed under linear combinations. This means that 

$$
\mathbf{Y} \sim N_{2}\left(\sqrt{2}\begin{pmatrix}
-1 \\
1 \\
\end{pmatrix}, \begin{pmatrix}
2 & 0 \\
0 & 4 \\
\end{pmatrix}\right)
$$ 

Moreover, since we know that uncorrelated variables in a random vector with a multivariate Gaussian distribution must be independent, this means that $Y_1$ and $Y_2$ are independent. 

Bonus: Calculations in R.

```{r}
mu.X <- c(0,2)
Sigma.X <- matrix(c(3,1,1,3), ncol = 2)
A <- matrix(c(1/sqrt(2), 1/sqrt(2), -1/sqrt(2), 1/sqrt(2)), ncol = 2)

mu.Y <- A %*% mu.X
mu.Y

Sigma.Y <- A %*% Sigma.X %*% t(A)
Sigma.Y  

# Yes, the coordinates of Y are independent, since Y is normally distributed 
# and the coordinates are uncorrelated.
```


## b)

By using the method of diagonalization one can show that the contours of a vector that has a multivariate Gaussian distribution are ellipsoids. Moreover, one can show that the axes of the ellipsoids have direction along the eigenvector of the covariance matrix $\Sigma$ and half-lengths $\sqrt{\lambda_i}b$ for, where $\lambda_i$ are the eigenvalues of $\Sigma$. More specifically, each of the half axes are given, in descending order based on length, by $\sqrt{\lambda_i}b$ with direction following each respective eigenvector $\mathbf{p}_i, \, i = 1, \dots, k$ for $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_k$, where $k$ is the dimension of the multivariate Gaussian distribution. 

The derivation can be done here, if this is something that is sought after (I will ask today).

MANGLER TEGNINGEN (med alle features markert) HER, FOR JEG KLARTE IKKE Å TEGNE FIGUREN I R LOL. Kan ev bare tegne for hånd sikkert. 

The probability that $\boldsymbol{X}$ falls within the given ellipse is

$$
\begin{split}
&P\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2} \leq b^2\right), \quad \text{where the change of variables } P\mathbf{y} = \mathbf{x}-\mathbf{\mu} \quad \text{is implemented} \\
= &P\left(\frac{y_1^2\lambda_2 + y_2^2\lambda_1}{\lambda_1\lambda_2} \leq 4.6^2\right) \quad \text{Denne er sikkert unødvendig}\\
= &\text{Tror ikke dette fører noe sted.}
\end{split}
$$

Theorem B8 in FKLM states that, in general, $\boldsymbol{Y} = (\boldsymbol{X}-\boldsymbol{\mu})\Sigma^{-1}(\boldsymbol{X}-\boldsymbol{\mu}) \sim \chi^2_2$ when $\boldsymbol{Y} \sim N_{2}(\mu, \Sigma)$. Hence, the probability that $\boldsymbol{X}$ falls within the given ellipse is $\approx 0.9$, which is seen from the given output of `qchisq(0.9, 2)` $=$ `r qchisq(0.9, 2)`. This means that the quantile with the value $b \approx 4.6$ corresponds to the probability $\approx 0.9$.

COMMENT: Denne [stackoverflow](https://math.stackexchange.com/questions/3543987/multivariate-normal-probability-of-being-inside-ellipse) kan være til hjelp også (samme oppgave lol).

# Problem 2 \hspace{3mm} Distributional results for $\bar{X}$ and $S^2$ for a univariate normal sample

## a)

$$
\overline{X} = \frac1n\boldsymbol{1}^T\boldsymbol{X} = \frac1n\begin{pmatrix} 
    1 \, \cdots \, 1
    \end{pmatrix}\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix}=\frac1n\sum_{i=1}^nX_i.
$$

Considering the $i^{th}$ component of $\boldsymbol{X}^TC\boldsymbol{X}$ gives (gidder ikke mer nå)

$$
\begin{split}
\begin{pmatrix} 
    X_1 \, \cdots \, X_n
\end{pmatrix}(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)_i\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix}
\end{split}
$$

We could also, instead of going the route above, show that $\frac1n\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{X} = \sum_{i=1}^n\overline{X} (?)$ and use the proof below. This can be shown by

$$
\frac1n\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{X} = \frac1n\begin{pmatrix} 
    1-1/n & -1/n & \dots & -1/n\\
    \vdots & \ddots & \vdots & \vdots \\
    -1/n & -1/n & \dots & 1-1/n
    \end{pmatrix}\begin{pmatrix} 
    X_1\\
    \vdots \\
    X_n
    \end{pmatrix} = 
$$

$$
\begin{split}
S^2 &= \frac{1}{n-1}\boldsymbol{X}^TC\boldsymbol{X} = \frac{1}{n-1}\boldsymbol{X}^T(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X} \\
&= \frac{1}{n-1}((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X})^T((I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)\boldsymbol{X}) \\
&= \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})(X_i-\overline{X}) = \frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2,
\end{split}
$$

since $(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)$ is idempotent and symmetric. The fact that $C$ is idempotent and symmetric may be verified by direct calculation. The idempotent property follows by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)^2 & = (I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T) = I - \frac2n\boldsymbol{1}\boldsymbol{1}^T + \frac{1}{n^2}\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T \\
&= I - \frac2n\boldsymbol{1}\boldsymbol{1}^T + \frac1n\boldsymbol{1}\boldsymbol{1}^T = I - \boldsymbol{1}\boldsymbol{1}^T, 
\end{split}
$$

where the third equality follows since, in $\mathbb{R}^n$, we have that $\boldsymbol{1}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T = n\boldsymbol{1}\boldsymbol{1}^T$. The symmetric property follows by

$$
\begin{split}
(I-\frac1n\boldsymbol{1}\boldsymbol{1}^T)^T = I^T-\frac1n(\boldsymbol{1}\boldsymbol{1}^T)^T = I-\frac1n\boldsymbol{1}\boldsymbol{1}^T.
\end{split}
$$

Hence, the proof above is correct based on the (correct) assumption that the centering matrix is idempotent and symmetric. 


DETTE ER LITT KAOS NÅ, MEN KAN FIKSES OPP I SENERE. 

## b)

$$
\frac{1}{n}\boldsymbol{1}^TC=\frac{1}{n}\boldsymbol{1}^TI-\frac{1}{n^2}\boldsymbol{1}^T\boldsymbol{1}\boldsymbol{1}^T=\boldsymbol{1}^T\frac{1}{n}-\frac{1}{n^2}\left[n\: ...\: n\right]=\boldsymbol{1}^T(1/n-1/n)=\boldsymbol{0}^T
$$



